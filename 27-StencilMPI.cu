#include <wb.h>
#include <mpi/mpi.h>

/*
 *  Constants
 */

const bool DUMP_DATA = false;

#define TIME_STEPS 499

const int INIT_TAG = 1000;
const int COMM1_TAG = 2000;
const int COMM2_TAG = 3000;
const int COLLECT_TAG = 4000;

#define C 0.00015
#define Y 0.065
#define X 0.065
#define Z 0.065

#define BLOCK_SIZE 16

#define DEBUG wbLog(TRACE, "On line ", __LINE__)

/*
 *  Type definitions
 */

enum Mode { GPU_SIMPLE = 1, GPU_OVERLAP };

struct JobDescriptor {
  int gpus;  // Total number of GPUs (domains)
  int gpuId; // GPU id used by this job
  int pid;   // MPI process id
  Mode mode; // Stencil implementation to use (simple or overlapped)

  float fac; // Numeric factor used in the stencil formula

  unsigned int nx; // X dimension size including halos
  unsigned int ny; // Y dimension size including halos
  unsigned int nz; // Z dimension size including halos

  size_t size;       // Size in elements of the input domain handled by this job
                     //  (includes halos).
  size_t outputSize; // Size in elements of the output domain generated by this
                     // job.
  size_t
      inputOff; // Offset in the input volume of the front Z halo plane of the
                //  domain processed by this job.
  size_t
      outputOff; // Offset in the output volume of the first Z plane generated
                 //  by this job.
};

/*
 *  Globals
 */

int pid;

/*
 *  Error checking and logging macros
 */

const unsigned msg_aux_size = 1000;
char msg_aux[msg_aux_size];

#define INFO(_msg, ...)                                                        \
  do {                                                                         \
    snprintf(msg_aux, msg_aux_size, _msg, ##__VA_ARGS__);                      \
    wbLog(INFO, msg_aux);                                                      \
  } while (0)

#define FATAL(_msg, ...)                                                       \
  do {                                                                         \
    snprintf(msg_aux, msg_aux_size, _msg, ##__VA_ARGS__);                      \
    wbLog(FATAL, msg_aux);                                                     \
    MPI_Finalize();                                                            \
    exit(1);                                                                   \
  } while (0)

#define CUDA_ERR(_err, _msg, ...)                                              \
  do {                                                                         \
    if ((_err) != cudaSuccess) {                                               \
      FATAL((_msg), ##__VA_ARGS__);                                            \
    }                                                                          \
  } while (0)

#define MPI_ERR(_err, _msg, ...)                                               \
  do {                                                                         \
    if ((_err) != MPI_SUCCESS) {                                               \
      FATAL((_msg), ##__VA_ARGS__);                                            \
    }                                                                          \
  } while (0)

/*
 *  Kernel
 */

#define Index3D(_i, _j, _k) ((_i) + nx * ((_j) + ny * (_k)))

__global__ void block2D_stencil_kernel(float fac, const float *A0, float *Anext,
                                       int nx, int ny, int nz) {
  int i = blockIdx.x * blockDim.x + threadIdx.x;
  int j = blockIdx.y * blockDim.y + threadIdx.y;

  if (i > 0 && j > 0 && (i < nx - 1) && (j < ny - 1)) {
    for (int k = 1; k < nz - 1; k++) /* For each frame along the z-axis */
    {
      Anext[Index3D(i, j, k)] =
          Z * (A0[Index3D(i, j, k + 1)] + A0[Index3D(i, j, k - 1)]) +
          Y * (A0[Index3D(i, j + 1, k)] + A0[Index3D(i, j - 1, k)]) +
          X * (A0[Index3D(i + 1, j, k)] + A0[Index3D(i - 1, j, k)]) -
          C * A0[Index3D(i, j, k)] / (fac * fac);
    }
  }
}

/*
 *  Host functions
 */

void block2D_stencil(float fac,           // Numeric factor
                     const float *A0,     // Input volume
                     float *Anext,        // Output volume
                     int nx,              // X dimension size including halos
                     int ny,              // Y dimension size including halos
                     int nz,              // Z dimension size including halos
                     cudaStream_t stream) // CUDA stream for the kernel launch
{
  int tx = BLOCK_SIZE;
  int ty = BLOCK_SIZE;

  dim3 block(tx, ty, 1);
  dim3 grid(((nx) + tx - 1) / tx, (ny + ty - 1) / ty, 1);

  block2D_stencil_kernel<<<grid, block, 0, stream>>>
      (fac, A0, Anext, nx, ny, nz);
}

void do_stencil_overlap(JobDescriptor *job, float *h_A0, float *h_Anext) {

  //@@ INSERT CODE HERE

}

float rand_float(float a, float b) {
  float random = ((float)rand()) / (float)RAND_MAX;
  float diff = b - a;
  float r = random * diff;
  return a + r;
}

int generate_data(float *A, int nx, int ny, int nz, unsigned int seed) {
  srand(seed);
  int s = 0;
  for (int i = 0; i < nz; ++i) {
    for (int j = 0; j < ny; ++j) {
      for (int k = 0; k < nx; ++k) {
        A[s] = rand_float(-100.f, 100.f);
        s++;
      }
    }
  }
  return 0;
}

char *getString(Mode mode) {
  switch (mode) {
  case GPU_SIMPLE:
    return (char *)"GPU_SIMPLE";
  case GPU_OVERLAP:
    return (char *)"GPU_OVERLAP";
  default:
    return (char *)"<unknown mode>";
  }
}

void do_stencil(JobDescriptor *job, float *h_A0, float *h_Anext) {
  MPI_Status status;
  int errMPI;

  float *d_A0;
  float *d_Anext;

  cudaError_t err;
  int gpus;

  err = cudaGetDeviceCount(&gpus);
  CUDA_ERR(err, "Error getting number of devices in process %u", pid);

  // Move to the right device
  err = cudaSetDevice(job->gpuId % gpus);
  CUDA_ERR(err, "Error setting device %u in process %u", job->gpuId % gpus,
           pid);

  ////////////////////////////////////////////////////
  // Allocate device 3D volumes for the current domain
  ////////////////////////////////////////////////////
  err = cudaMalloc((void **)&d_A0, job->size * sizeof(float));
  CUDA_ERR(err, "Error allocating device memory for volume 1 in process %u",
           pid);

  err = cudaMalloc((void **)&d_Anext, job->size * sizeof(float));
  CUDA_ERR(err, "Error allocating device memory for volume 2 in process %u",
           pid);

  ///////////////////////////////
  // Initialize device 3D volumes
  ///////////////////////////////
  err = cudaMemcpy(d_A0, h_A0, job->size * sizeof(float), cudaMemcpyDefault);
  CUDA_ERR(err, "Error initializing volume 1 in process %u", pid);

  err = cudaMemset(d_Anext, 0, job->size * sizeof(float));
  CUDA_ERR(err, "Error memsetting next volume in process %u", pid);

  int tx = BLOCK_SIZE;
  int ty = BLOCK_SIZE;

  dim3 block(tx, ty, 1);
  dim3 grid(((job->nx) + tx - 1) / tx, (job->ny + ty - 1) / ty, 1);

  // Loop per each time-step
  for (int t = 0; t < TIME_STEPS; ++t) {
    // Call the kernel
    block2D_stencil(job->fac, d_A0, d_Anext, job->nx, job->ny, job->nz, 0);

    err = cudaThreadSynchronize();
    CUDA_ERR(err, "Error launching kernel in process %u", pid);

    if (job->gpus > 1) {
      /////////////////////////
      // Send data to neighbors
      /////////////////////////

      // LEFT
      if (job->gpuId != 0) {
        err = cudaMemcpy(h_Anext, d_Anext + job->nx * job->ny,
                         job->nx * job->ny * sizeof(float), cudaMemcpyDefault);
        CUDA_ERR(err, "Error copying to prev in process %u", pid);

        errMPI = MPI_Sendrecv(h_Anext, job->nx * job->ny, MPI_FLOAT, pid - 1,
                              COMM1_TAG, h_Anext + job->nx * job->ny,
                              job->nx * job->ny, MPI_FLOAT, pid - 1, COMM2_TAG,
                              MPI_COMM_WORLD, &status);
        MPI_ERR(errMPI, "Error sendrecv left in process %u", pid);

        err = cudaMemcpy(d_Anext, h_Anext + job->nx * job->ny,
                         job->nx * job->ny * sizeof(float), cudaMemcpyDefault);
        CUDA_ERR(err, "Error copying to prev in process %u", pid);
      }

      // RIGHT
      if (job->gpuId < job->gpus - 1) {
        err = cudaMemcpy(h_Anext, d_Anext + job->size - 2 * job->nx * job->ny,
                         job->nx * job->ny * sizeof(float), cudaMemcpyDefault);
        CUDA_ERR(err, "Error copying to next in process %u", pid);

        errMPI = MPI_Sendrecv(h_Anext, job->nx * job->ny, MPI_FLOAT, pid + 1,
                              COMM2_TAG, h_Anext + job->nx * job->ny,
                              job->nx * job->ny, MPI_FLOAT, pid + 1, COMM1_TAG,
                              MPI_COMM_WORLD, &status);
        MPI_ERR(errMPI, "Error sendrecv right in process %u", pid);

        err = cudaMemcpy(d_Anext + job->size - job->nx * job->ny,
                         h_Anext + job->nx * job->ny,
                         job->nx * job->ny * sizeof(float), cudaMemcpyDefault);
        CUDA_ERR(err, "Error copying to next in process %u", pid);
      }
    }

    if (t < TIME_STEPS - 1) {
      // Update pointers
      float *tmp;
      tmp = d_Anext;
      d_Anext = d_A0;
      d_A0 = tmp;
    }
  }

  err = cudaMemcpy(h_Anext, d_Anext, job->size * sizeof(float),
                   cudaMemcpyDefault);
  CUDA_ERR(err, "Error copying results to host memory in process %u", pid);

  // Free device memory
  cudaFree(d_A0);
  cudaFree(d_Anext);
}

int main(int argc, char *argv[]) {
  wbArg_t args;
  int nx, ny, nz;
  float fac;
  int nprocs = -1;
  int errMPI;
  MPI_Status status;

  args = wbArg_read(argc, argv);

  errMPI = MPI_Comm_rank(MPI_COMM_WORLD, &pid);
  MPI_ERR(errMPI, "Error getting MPI rank");

  errMPI = MPI_Comm_size(MPI_COMM_WORLD, &nprocs);
  MPI_ERR(errMPI, "Error getting MPI size");

  unsigned int domains = nprocs - 1;

  if (pid == 0) {
    //////////////////
    // Manager process
    //

    Mode mode;
    unsigned int seed;
    int *input_desc;
    int num_input_desc;

    wbTime_start(Generic, "Generating data and allocating memory on host");
    mode = (Mode)wbImport_flag(wbArg_getInputFile(args, 0));
    input_desc = (int *)wbImport(wbArg_getInputFile(args, 1), &num_input_desc,
                                 "Integer");

    if (num_input_desc != 4) {
      FATAL("Expected 4 input description params (3 dim sizes + seed), got %d",
            num_input_desc);
    } else {
      nx = input_desc[0];
      ny = input_desc[1];
      nz = input_desc[2];
      seed = input_desc[3];
    }
    if (nx < 1 || ny < 1 || nz < 1) {
      FATAL("Invalid volume size %d x %d x %d", nx, ny, nz);
    }

    wbLog(INFO, "Mode ", getString(mode), " (", mode, ")");
    INFO("Volume size %d x %d x %d", nx, ny, nz);
    INFO("Seed %u", seed);

    JobDescriptor *descriptors = new JobDescriptor[nprocs];

    const size_t size = nx * ny * nz;

    float *h_input = (float *)malloc(size * sizeof(float));
    if (h_input == NULL) {
      FATAL("Error allocating input volume");
    }
    float *h_out_gpu = (float *)malloc(size * sizeof(float));
    if (h_out_gpu == NULL) {
      FATAL("Error allocating output volume");
    }

    generate_data(h_input, nx, ny, nz, seed);
    if (DUMP_DATA) {
      FILE *file = fopen("in.bin", "w");
      if (!file) {
        fprintf(stderr, "Error opening file 'in.bin'");
      } else {
        size_t written;
        written = fwrite(h_input, sizeof(float), nx * ny * nz, file);
        if (written != nx * ny * nz) {
          fprintf(stderr, "Error writing to file 'in.bin'");
        }
        fclose(file);
      }
    }
    memset(h_out_gpu, 0, size * sizeof(float));

    fac = h_input[0];
    wbTime_stop(Generic, "Generating data and allocating memory on host");

    wbTime_start(Generic, "Distributing work to worker MPI processes");
    // Create one job descriptor per domain
    for (unsigned int d = 0; d < domains; ++d) {
      descriptors[d] = JobDescriptor();
      JobDescriptor *curr_desc = &descriptors[d];
      curr_desc->gpus = domains;
      curr_desc->gpuId = d;
      curr_desc->pid = d + 1;
      curr_desc->fac = fac;
      curr_desc->nx = nx;
      curr_desc->ny = ny;
      curr_desc->nz = (nz - 2) / domains + 2;
      curr_desc->mode = mode;

      if (d == domains - 1) {
        curr_desc->nz += (nz - 2) % domains;
      }

      curr_desc->size = nx * ny * curr_desc->nz;
      curr_desc->outputSize = nx * ny * (curr_desc->nz - 2);

      if (d == 0) {
        curr_desc->outputSize += nx * ny;
      }
      if (d == domains - 1) {
        curr_desc->outputSize += nx * ny;
      }

      if (d == 0) {
        curr_desc->inputOff = 0;
        curr_desc->outputOff = 0;
      } else {
        curr_desc->inputOff =
            descriptors[d - 1].inputOff + nx * ny * (descriptors[d - 1].nz - 2);
        curr_desc->outputOff =
            descriptors[d - 1].outputOff + descriptors[d - 1].outputSize;
      }

      // Send descriptor to de appropriate MPI process
      errMPI = MPI_Send(curr_desc, sizeof(JobDescriptor), MPI_BYTE,
                        curr_desc->pid, INIT_TAG, MPI_COMM_WORLD);
      MPI_ERR(errMPI, "Error sending descriptor to process %u", curr_desc->pid);

      // Send initial contents of the simulation
      errMPI = MPI_Send(h_input + curr_desc->inputOff, curr_desc->size,
                        MPI_FLOAT, curr_desc->pid, INIT_TAG, MPI_COMM_WORLD);
      MPI_ERR(errMPI, "Error sending initial data to process %u",
              curr_desc->pid);
    }

    // Send fac value to all worker processes
    errMPI = MPI_Bcast(&fac, 1, MPI_FLOAT, 0, MPI_COMM_WORLD);
    MPI_ERR(errMPI, "Error broadcasting fac");

    wbTime_stop(Generic, "Distributing work to worker MPI processes");

    wbTime_start(Compute, "Performing stencil computation");

    // Receive the output of each worker process (domain)
    for (unsigned int d = 0; d < domains; ++d) {
      //fprintf(stderr, "%u, %u\n", descriptors[d].outputOff, descriptors[d].outputSize);
      errMPI = MPI_Recv(
          &h_out_gpu[ descriptors[d].outputOff], descriptors[d].outputSize,
          MPI_FLOAT, descriptors[d].pid, COLLECT_TAG, MPI_COMM_WORLD, &status);
      MPI_ERR(errMPI, "Error receiving output data from process %u",
              descriptors[d].pid);
    }
    wbTime_stop(Compute, "Performing stencil computation");

    const unsigned int chsum_size = 16;
    unsigned char chsum[chsum_size];
    int chsum_ints[chsum_size];
    wbMD5((const unsigned char *)h_out_gpu, (size) * sizeof(float), chsum);
    printf("Result MD5: ");
    for (int i = 0; i < chsum_size; i++) {
      printf("%02x", chsum[i]);
      chsum_ints[i] = chsum[i];
    }
    printf("\n");
    wbSolution(args, chsum_ints, chsum_size);

    delete[] descriptors;

    free(h_input);
    free(h_out_gpu);


  } else {

    ///////////////////
    // Worker processes
    //
    JobDescriptor descriptor;
    cudaError_t err;
    float *h_input = NULL;
    float *h_out = NULL;

    // Descriptor
    // Receive job descriptor for the current MPI process
    errMPI = MPI_Recv(&descriptor, sizeof(JobDescriptor), MPI_BYTE, 0, INIT_TAG,
                      MPI_COMM_WORLD, &status);
    MPI_ERR(errMPI, "Error receiving job descriptor in process %u", pid);

    int gpus;

    err = cudaGetDeviceCount(&gpus);
    CUDA_ERR(err, "Error getting number of devices in process %u", pid);

    // Move to the right device
    err = cudaSetDevice(descriptor.gpuId % gpus);
    CUDA_ERR(err, "Error setting device %u in process %u",
             descriptor.gpuId % gpus, pid);

    //////////////////////////////////////////////////
    // Allocate host 3D volumes for the current domain
    //////////////////////////////////////////////////
    h_input = (float *)malloc(descriptor.size * sizeof(float));
    h_out = (float *)malloc(descriptor.size * sizeof(float));

    // Receive initial data
    errMPI = MPI_Recv(h_input, descriptor.size, MPI_FLOAT, 0, INIT_TAG,
                      MPI_COMM_WORLD, &status);
    MPI_ERR(errMPI, "Error receiving initial data in process %u", pid);

    // Receive fac value
    errMPI = MPI_Bcast(&fac, 1, MPI_FLOAT, 0, MPI_COMM_WORLD);
    MPI_ERR(errMPI, "Error receiving fac value in process %u", pid);

    ////////////////////////////
    // Perform local computation
    ////////////////////////////

    wbTime_start(Compute, "Worker performing stencil computation");
    if (descriptor.mode == GPU_SIMPLE) {
      do_stencil(&descriptor, h_input, h_out);
    } else if (descriptor.mode == GPU_OVERLAP) {
      do_stencil_overlap(&descriptor, h_input, h_out);
    } else {
      FATAL("Invalid mode!");
    }
    wbTime_stop(Compute, "Worker performing stencil computation");

    size_t localOff = 0;
    if (pid != 1) {
      localOff = descriptor.nx * descriptor.ny;
    }

    // Send output to the main process
    fprintf(stderr, "rank: %d, %lu, %lu, %lu\n", pid, localOff, descriptor.size, descriptor.outputSize);
    errMPI = MPI_Send(h_out + localOff, descriptor.outputSize, MPI_FLOAT, 0,
                      COLLECT_TAG, MPI_COMM_WORLD);
    MPI_ERR(errMPI, "Error sending output values in process %u", pid);

    /////////////////////////////////////////////
    // Free host 3D volumes of the current domain
    /////////////////////////////////////////////
    free(h_input);
    free(h_out);

  }

  MPI_Barrier(MPI_COMM_WORLD);

  errMPI = MPI_Finalize();
  MPI_ERR(errMPI, "Error finalizing MPI in process %u", pid);
  return 0;
}
